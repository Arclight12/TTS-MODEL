# Date: 16-07-2025

## ‚úÖ Today's Work

- Today, we tested and ran models from a few **different libraries** to check if any of them can **convert phonemes to audio correctly**.  
- The models tested were **VITS** and **FastPitch**.  
- We ran the **FastPitch model** from the **NeMo library (NVIDIA)**.  
- For **VITS**, we tried multiple implementations from **ESPnet** and **Coqui TTS**.  
- All these models were able to produce audio, but **none were intelligible or proper when given phoneme input**.  

---

## üìå Findings  

1. So far, we have checked **three libraries** and **multiple models**, and all have the **same issue**:  
   - None were able to produce **proper audio** when **phonemes** were given as input.  

2. The main problem seems to be that **most pre-trained models are not trained on phonemes as input**.  
   - While they *accept* phonemes as input, they **can‚Äôt produce proper output** because they learned **text-to-audio mappings**, not **phoneme-to-audio mappings**.  

3. We also tested **FastPitch** (a more advanced version of FastSpeech2) from NVIDIA‚Äôs NeMo library.  
   - However, even this model was **not able to produce correct audio** with phoneme input.  

4. According to online sources, **ESPnet** previously had models trained with **phoneme input**,  
   - But many of these models have either been **discontinued** or **restricted** for access.  

---

## üìù Notes  

### üéº FastPitch vs FastSpeech2 ‚Äì Easy Difference  

- **FastSpeech2**  
  - A **non-autoregressive** TTS model.  
  - Predicts **duration, pitch, and energy** before generating the mel-spectrogram.  
  - Needs an **aligner** during training.  

- **FastPitch**  
  - Based on **FastSpeech2**, but adds **better pitch prediction** for improved tone and naturalness.  
  - Also faster and more controllable.  

‚úÖ **Key takeaway:** FastPitch = FastSpeech2 **+ better pitch handling**.  

---

### üèóÔ∏è Architecture of Different Models (Simple Overview)  

Here‚Äôs how the main TTS models are built, in **simple terms**:  

---

#### 1Ô∏è‚É£ Tacotron2  
- Uses a **Seq2Seq model with attention** (like an RNN-based translator).  
- Generates mel-spectrogram **step by step** ‚Üí needs a **vocoder** (like HiFi-GAN) to produce audio.  
‚úÖ Natural sounding but **slow** because it‚Äôs autoregressive.  

---

#### 2Ô∏è‚É£ Glow-TTS  
- Uses a **flow-based model** to map text directly to mel-spectrograms.  
- Predicts **duration explicitly**, so no attention needed.  
‚úÖ Faster and more stable than Tacotron2, but still needs a **vocoder**.  

---

#### 3Ô∏è‚É£ VITS  
- Combines several techniques (**VAE + Flow + GAN**) in **one end-to-end model**.  
- Directly generates **waveforms** from text/phonemes ‚Üí **no separate vocoder needed**.  
‚úÖ High quality, end-to-end. ‚ùå But more complex to train.  

---

#### 4Ô∏è‚É£ FastSpeech2  
- Built on a **Transformer** (like in NLP).  
- Predicts **duration, pitch, and energy**, then generates mel-spectrogram **in parallel** (not step by step).  
- Needs a **vocoder** for final audio.  
‚úÖ Much faster & stable compared to Tacotron2.  

---

#### 5Ô∏è‚É£ FastPitch  
- Same **Transformer backbone** as FastSpeech2.  
- Adds **pitch-aware prediction** for better intonation control.  
‚úÖ Faster + better pitch control.  

---

‚úÖ **Quick Summary Table**  

| Model        | Autoregressive? | Core Idea             | Needs Vocoder? |
|--------------|-----------------|-----------------------|---------------|
| **Tacotron2** | ‚úÖ Yes          | Seq2Seq + Attention   | ‚úÖ Yes |
| **Glow-TTS**  | ‚ùå No           | Flow-based model      | ‚úÖ Yes |
| **VITS**      | ‚ùå No           | VAE + Flow + GAN      | ‚ùå No |
| **FastSpeech2** | ‚ùå No         | Transformer-based     | ‚úÖ Yes |
| **FastPitch**  | ‚ùå No          | Transformer + Pitch   | ‚úÖ Yes |

---

‚úÖ **In simple words:**  
- **Tacotron2** ‚Üí Old-style **Seq2Seq**, slow but natural.  
- **FastSpeech2 & FastPitch** ‚Üí **Transformer-based**, very fast.  
- **Glow-TTS** ‚Üí Uses **flow models**, also fast.  
- **VITS** ‚Üí Fully end-to-end, no vocoder needed.  

---

